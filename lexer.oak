use std.io

struct Pos {
   path Str
   row int
   col int
}

fn [<<](f *File, pos Pos) *File {
   return f << pos.path << ':' << pos.row << ':' << pos.col << ": "
}

alias Token_Kind int
const (
   TOKEN_EOF
   TOKEN_IDENT
   TOKEN_DEF
   TOKEN_COMMA
   TOKEN_LPAREN
   TOKEN_RPAREN
   TOKEN_EQ
   COUNT_TOKENS
)

assert COUNT_TOKENS == 7
fn str_from_token_kind(kind Token_Kind) Str {
   match kind {
      TOKEN_EOF => return "end of file"
      TOKEN_IDENT => return "identifier"
      TOKEN_DEF => return "'::'"
      TOKEN_COMMA => return "','"
      TOKEN_LPAREN => return "'('"
      TOKEN_RPAREN => return "')'"
      TOKEN_EQ => return "'='"
      else => assert false
   }
   return ""
}

struct Token {
   kind Token_Kind
   data int
   pos Pos
   str Str
}

struct Lexer {
   pos Pos
   str Str
   peeked bool
   buffer Token
   prev_row int
}

let lexer Lexer

fn lexer_open(path *char) bool {
   lexer.pos.path = str_from_cstr(path)

   if !read_file(&lexer.str, path) {
      if lexer.peeked {
         &stderr << lexer.buffer.pos
      }

      &stderr << "error: could not read file '" << lexer.pos.path << "'\n"
      return false
   }

   lexer.pos.row = 1
   lexer.pos.col = 1
   lexer.peeked = false
   return true
}

fn lexer_from_str(str Str, file Str) {
   lexer.pos.path = file
   lexer.str = str
   lexer.pos.row = 1
   lexer.pos.col = 1
   lexer.peeked = false
}

fn lexer_buffer(token Token) {
   lexer.peeked = true
   lexer.buffer = token
}

fn lexer_advance() {
   if *lexer.str.data == '\n' {
      lexer.pos.row += 1
      lexer.pos.col = 1
   } else {
      lexer.pos.col += 1
   }

   lexer.str.data += 1 as *char
   lexer.str.size -= 1
}

fn lexer_consume() char {
   lexer_advance()
   return lexer.str.data[-1]
}

fn lexer_match(ch char) bool {
   if lexer.str.size > 0 && *lexer.str.data == ch {
      lexer_advance()
      return true
   }
   return false
}

fn error_invalid(name Str) {
   &stderr << lexer.pos << "error: invalid " << name << " '" << lexer.str.data[-1] << "'\n"
}

assert COUNT_TOKENS == 7
fn lexer_next(result *Token) bool {
   if lexer.peeked {
      lexer.peeked = false
      lexer.prev_row = lexer.buffer.pos.row
      *result = lexer.buffer
      return true
   }

   for lexer.str.size > 0 {
      if isspace(*lexer.str.data) {
         lexer_advance()
      } else if lexer_match('#') {
         if lexer_match('#') {
            for lexer.str.size > 0 {
               if lexer_match('#') && lexer_match('#') {
                  break
               }

               lexer_advance()
            }
         } else {
            for lexer.str.size > 0 && *lexer.str.data != '\n' {
               lexer_advance()
            }
         }
      } else {
         break
      }
   }

   let token Token
   token.pos = lexer.pos
   token.str = lexer.str

   if lexer.str.size == 0 {
      token.kind = TOKEN_EOF
   } else if isalpha(*lexer.str.data) || *lexer.str.data == '_' {
      for lexer.str.size > 0 && (isalnum(*lexer.str.data) || *lexer.str.data == '_') {
         lexer_advance()
      }

      token.str.size -= lexer.str.size

      token.kind = TOKEN_IDENT
   } else {
      match lexer_consume() {
         ':' => {
            if lexer_match(':') {
               token.kind = TOKEN_DEF
            }
         }
         ',' => token.kind = TOKEN_COMMA
         '(' => token.kind = TOKEN_LPAREN
         ')' => token.kind = TOKEN_RPAREN
         '=' => token.kind = TOKEN_EQ
         else => {
            lexer.pos = token.pos
            error_invalid("character")
            return false
         }
      }

      token.str.size -= lexer.str.size
   }

   lexer.prev_row = token.pos.row
   *result = token
   return true
}

fn lexer_peek(result *Token) bool {
   let prev_row = lexer.prev_row
   if !lexer.peeked {
      let token Token
      if !lexer_next(&token) { return false }
      lexer_buffer(token)
      lexer.prev_row = prev_row
   }
   *result = lexer.buffer
   return true
}

fn lexer_read(kind Token_Kind, result *bool) bool {
   let token Token
   if !lexer_peek(&token) { return false }
   lexer.peeked = lexer.buffer.kind != kind
   *result = !lexer.peeked
   return true
}

fn lexer_expect(kind Token_Kind, result *Token) bool {
   let token Token
   if !lexer_next(&token) { return false }
   if token.kind != kind {
      &stderr << token.pos << "error: expected " << str_from_token_kind(kind) << ", got " << str_from_token_kind(token.kind) << "\n"
      return false
   }
   *result = token
   return true
}

fn lexer_either(a Token_Kind, b Token_Kind, result *Token) bool {
   let token Token
   if !lexer_next(&token) { return false }
   if token.kind != a && token.kind != b {
      &stderr << token.pos << "error: expected " << str_from_token_kind(a) << " or " << str_from_token_kind(b) << ", got " << str_from_token_kind(token.kind) << "\n"
      return false
   }
   *result = token
   return true
}

fn lexer_peek_row(token *Token, result *bool) bool {
   if !lexer_peek(token) { return false }
   *result = token.pos.row == lexer.prev_row
   return true
}
