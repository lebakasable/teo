use std.io

struct Pos {
   path Str
   row int
   col int
}

fn [<<](f *File, pos Pos) *File {
   return f << pos.path << ':' << pos.row << ':' << pos.col << ": "
}

alias Token_Kind int
const (
   TOKEN_EOF
   TOKEN_IDENT
   TOKEN_COMMA
   TOKEN_LPAREN
   TOKEN_RPAREN
   TOKEN_EQ
   COUNT_TOKENS
)

assert COUNT_TOKENS == 6
fn str_from_token_kind(kind Token_Kind) Str {
   match kind {
      TOKEN_EOF => return "end of file"
      TOKEN_IDENT => return "identifier"
      TOKEN_COMMA => return "','"
      TOKEN_LPAREN => return "'('"
      TOKEN_RPAREN => return "')'"
      TOKEN_EQ => return "'='"
      else => assert false
   }
   return ""
}

struct Token {
   kind Token_Kind
   data int
   pos Pos
   str Str
}

struct Lexer {
   pos Pos
   str Str
   peeked bool
   buffer Token
   prev_row int
}

let lexer Lexer

fn lexer_open(path *char) {
   lexer.pos.path = str_from_cstr(path)

   if !read_file(&lexer.str, path) {
      if lexer.peeked {
         &stderr << lexer.buffer.pos
      }

      &stderr << "error: could not read file '" << lexer.pos.path << "'\n"
      exit(1)
   }

   lexer.pos.row = 1
   lexer.pos.col = 1
   lexer.peeked = false
}

fn lexer_buffer(token Token) {
   lexer.peeked = true
   lexer.buffer = token
}

fn lexer_advance() {
   if *lexer.str.data == '\n' {
      lexer.pos.row += 1
      lexer.pos.col = 1
   } else {
      lexer.pos.col += 1
   }

   lexer.str.data += 1 as *char
   lexer.str.size -= 1
}

fn lexer_consume() char {
   lexer_advance()
   return lexer.str.data[-1]
}

fn lexer_match(ch char) bool {
   if lexer.str.size > 0 && *lexer.str.data == ch {
      lexer_advance()
      return true
   }
   return false
}

fn error_invalid(name Str) {
   &stderr << lexer.pos << "error: invalid " << name << " '" << lexer.str.data[-1] << "'\n"
   exit(1)
}

assert COUNT_TOKENS == 6
fn lexer_next() Token {
   if lexer.peeked {
      lexer.peeked = false
      lexer.prev_row = lexer.buffer.pos.row
      return lexer.buffer
   }

   for lexer.str.size > 0 {
      if isspace(*lexer.str.data) {
         lexer_advance()
      } else if lexer_match('#') {
         if lexer_match('#') {
            for lexer.str.size > 0 {
               if lexer_match('#') && lexer_match('#') {
                  break
               }

               lexer_advance()
            }
         } else {
            for lexer.str.size > 0 && *lexer.str.data != '\n' {
               lexer_advance()
            }
         }
      } else {
         break
      }
   }

   let token Token
   token.pos = lexer.pos
   token.str = lexer.str

   if lexer.str.size == 0 {
      token.kind = TOKEN_EOF
   } else if isalpha(*lexer.str.data) || *lexer.str.data == '_' {
      for lexer.str.size > 0 && (isalnum(*lexer.str.data) || *lexer.str.data == '_') {
         lexer_advance()
      }

      token.str.size -= lexer.str.size

      token.kind = TOKEN_IDENT
   } else {
      match lexer_consume() {
         ',' => token.kind = TOKEN_COMMA
         '(' => token.kind = TOKEN_LPAREN
         ')' => token.kind = TOKEN_RPAREN
         '=' => token.kind = TOKEN_EQ
         else => {
            lexer.pos = token.pos
            error_invalid("character")
         }
      }

      token.str.size -= lexer.str.size
   }

   lexer.prev_row = token.pos.row
   return token
}

fn lexer_peek() Token {
   let prev_row = lexer.prev_row
   if !lexer.peeked {
      lexer_buffer(lexer_next())
      lexer.prev_row = prev_row
   }
   return lexer.buffer
}

fn lexer_read(kind Token_Kind) bool {
   lexer_peek()
   lexer.peeked = lexer.buffer.kind != kind
   return !lexer.peeked
}

fn lexer_expect(kind Token_Kind) Token {
   let token = lexer_next()
   if token.kind != kind {
      &stderr << token.pos << "error: expected " << str_from_token_kind(kind) << ", got " << str_from_token_kind(token.kind) << "\n"
      exit(1)
   }
   return token
}

fn lexer_either(a Token_Kind, b Token_Kind) Token {
   let token = lexer_next()
   if token.kind != a && token.kind != b {
      &stderr << token.pos << "error: expected " << str_from_token_kind(a) << " or " << str_from_token_kind(b) << ", got " << str_from_token_kind(token.kind) << "\n"
      exit(1)
   }
   return token
}

fn lexer_peek_row(token *Token) bool {
   *token = lexer_peek()
   return token.pos.row == lexer.prev_row
}
