use std.io
use src.common

struct Pos {
   path Str
   row int
   col int
}

fn [<<](f *File, pos Pos) *File {
   if in_repl {
      for let i = 0, i < prompt.size + pos.col - 1, i += 1 {
         f << " "
      }
      f << "^\n"
   } else {
      f << pos.path << ':' << pos.row << ':' << pos.col << ": "
   }
   return f
}

alias Token_Kind int
const (
   TOKEN_EOF
   TOKEN_IDENT
   TOKEN_COMMA
   TOKEN_LPAREN
   TOKEN_RPAREN
   TOKEN_EQ
   TOKEN_RULE
   TOKEN_SHAPE
   TOKEN_APPLY
   TOKEN_DONE
   TOKEN_UNDO
   TOKEN_QUIT
   TOKEN_ALL
   TOKEN_FIRST
   TOKEN_ADD
   TOKEN_SUB
   TOKEN_MUL
   TOKEN_DIV
   TOKEN_POW
   COUNT_TOKENS
)

assert COUNT_TOKENS == 19
fn str_from_token_kind(kind Token_Kind) Str {
   match kind {
      TOKEN_EOF => return "end of file"
      TOKEN_IDENT => return "identifier"
      TOKEN_COMMA => return "','"
      TOKEN_LPAREN => return "'('"
      TOKEN_RPAREN => return "')'"
      TOKEN_EQ => return "'='"
      TOKEN_RULE => return "command 'rule'"
      TOKEN_SHAPE => return "command 'shape'"
      TOKEN_APPLY => return "command 'apply'"
      TOKEN_DONE => return "command 'done'"
      TOKEN_UNDO => return "command 'undo'"
      TOKEN_QUIT => return "command 'quit'"
      TOKEN_ALL => return "strategy 'all'"
      TOKEN_FIRST => return "strategy 'first'"
      TOKEN_ADD => return "'+'"
      TOKEN_SUB => return "'-'"
      TOKEN_MUL => return "'*'"
      TOKEN_DIV => return "'/'"
      TOKEN_POW => return "'^'"
      else => assert false
   }
   return ""
}

struct Token {
   kind Token_Kind
   data int
   pos Pos
   str Str
}

assert COUNT_TOKENS == 19
fn [<<](file *File, token Token) *File {
   if token.kind == TOKEN_IDENT {
      file << token.str
   } else {
      file << str_from_token_kind(token.kind)
   }
   return file
}

struct Lexer {
   pos Pos
   str Str
   peeked bool
   buffer Token
   prev_row int
}

let lexer Lexer

fn lexer_open(path *char) bool {
   lexer.pos.path = str_from_cstr(path)

   if !read_file(&lexer.str, path) {
      if lexer.peeked {
         &stderr << lexer.buffer.pos
      }

      &stderr << "error: could not read file '" << lexer.pos.path << "'\n"
      return false
   }

   lexer.pos.row = 1
   lexer.pos.col = 1
   lexer.peeked = false
   return true
}

fn lexer_from_str(str Str, file Str) {
   lexer.pos.path = file
   lexer.str = str
   lexer.pos.row = 1
   lexer.pos.col = 1
   lexer.peeked = false
}

fn lexer_buffer(token Token) {
   lexer.peeked = true
   lexer.buffer = token
}

fn lexer_advance() {
   if *lexer.str.data == '\n' {
      lexer.pos.row += 1
      lexer.pos.col = 1
   } else {
      lexer.pos.col += 1
   }

   lexer.str.data += 1 as *char
   lexer.str.size -= 1
}

fn lexer_consume() char {
   lexer_advance()
   return lexer.str.data[-1]
}

fn lexer_match(ch char) bool {
   if lexer.str.size > 0 && *lexer.str.data == ch {
      lexer_advance()
      return true
   }
   return false
}

fn error_invalid(name Str) {
   &stderr << lexer.pos << "error: invalid " << name << " '" << lexer.str.data[-1] << "'\n"
}

assert COUNT_TOKENS == 19
fn lexer_next(result *Token) bool {
   if lexer.peeked {
      lexer.peeked = false
      lexer.prev_row = lexer.buffer.pos.row
      *result = lexer.buffer
      return true
   }

   for lexer.str.size > 0 {
      if isspace(*lexer.str.data) {
         lexer_advance()
      } else if lexer_match('#') {
         if lexer_match('#') {
            for lexer.str.size > 0 {
               if lexer_match('#') && lexer_match('#') {
                  break
               }

               lexer_advance()
            }
         } else {
            for lexer.str.size > 0 && *lexer.str.data != '\n' {
               lexer_advance()
            }
         }
      } else {
         break
      }
   }

   let token Token
   token.pos = lexer.pos
   token.str = lexer.str

   if lexer.str.size == 0 {
      token.kind = TOKEN_EOF
   } else if isalnum(*lexer.str.data) || *lexer.str.data == '_' {
      for lexer.str.size > 0 && (isalnum(*lexer.str.data) || *lexer.str.data == '_') {
         lexer_advance()
      }

      token.str.size -= lexer.str.size

      if token.str == "rule" {
         token.kind = TOKEN_RULE
      } else if token.str == "shape" {
         token.kind = TOKEN_SHAPE
      } else if token.str == "apply" {
         token.kind = TOKEN_APPLY
      } else if token.str == "done" {
         token.kind = TOKEN_DONE
      } else if token.str == "undo" {
         token.kind = TOKEN_UNDO
      } else if token.str == "quit" {
         token.kind = TOKEN_QUIT
      } else if token.str == "all" {
         token.kind = TOKEN_ALL
      } else if token.str == "first" {
         token.kind = TOKEN_FIRST
      } else {
         token.kind = TOKEN_IDENT
      }
   } else {
      match lexer_consume() {
         ',' => token.kind = TOKEN_COMMA
         '(' => token.kind = TOKEN_LPAREN
         ')' => token.kind = TOKEN_RPAREN
         '=' => token.kind = TOKEN_EQ
         '+' => token.kind = TOKEN_ADD
         '-' => token.kind = TOKEN_SUB
         '*' => token.kind = TOKEN_MUL
         '/' => token.kind = TOKEN_DIV
         '^' => token.kind = TOKEN_POW
         else => {
            lexer.pos = token.pos
            error_invalid("character")
            return false
         }
      }

      token.str.size -= lexer.str.size
   }

   lexer.prev_row = token.pos.row
   *result = token
   return true
}

fn lexer_peek(result *Token) bool {
   let prev_row = lexer.prev_row
   if !lexer.peeked {
      let token Token
      if !lexer_next(&token) { return false }
      lexer_buffer(token)
      lexer.prev_row = prev_row
   }
   *result = lexer.buffer
   return true
}

fn lexer_read(kind Token_Kind, result *bool) bool {
   let token Token
   if !lexer_peek(&token) { return false }
   lexer.peeked = lexer.buffer.kind != kind
   *result = !lexer.peeked
   return true
}

fn lexer_expect(kind Token_Kind, result *Token) bool {
   let token Token
   if !lexer_next(&token) { return false }
   if token.kind != kind {
      &stderr << token.pos << "error: expected " << str_from_token_kind(kind) << ", got " << str_from_token_kind(token.kind) << "\n"
      return false
   }
   *result = token
   return true
}

fn lexer_either(a Token_Kind, b Token_Kind, result *Token) bool {
   let token Token
   if !lexer_next(&token) { return false }
   if token.kind != a && token.kind != b {
      &stderr << token.pos << "error: expected " << str_from_token_kind(a) << " or " << str_from_token_kind(b) << ", got " << str_from_token_kind(token.kind) << "\n"
      return false
   }
   *result = token
   return true
}

fn lexer_peek_row(token *Token, result *bool) bool {
   if !lexer_peek(token) { return false }
   *result = token.pos.row == lexer.prev_row
   return true
}
